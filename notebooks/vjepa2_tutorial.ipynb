{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V-JEPA 2-AC â€” Tutorial Notebook\n",
    "\n",
    "**Goal:** Make it simple to plug in a dataset and:\n",
    "- Understand inputs\n",
    "- Run **one-step** and **open-loop** predictions\n",
    "- Decode tokens back to images\n",
    "- Save nice **GT (ground-truth) vs. Pred** videos with **automatic naming** in `videos/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True | device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------\n",
    "# ðŸ“¦ Imports & Global Config\n",
    "# ----------------------------------------\n",
    "import os, sys, time, math, pickle, shutil, datetime, pathlib\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import imageio\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision.utils import make_grid\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "# Compute device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()} | device: {DEVICE}\")\n",
    "\n",
    "# Visualization defaults\n",
    "plt.rcParams[\"figure.figsize\"] = (5, 5)\n",
    "plt.rcParams[\"axes.grid\"] = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¦ V-JEPA 2-AC â€” What Data Does It Use?\n",
    "\n",
    "V-JEPA 2-AC is an **action-conditioned latent world model** trained to predict future features from robot interaction data. It takes three inputs:\n",
    "\n",
    "### ðŸ–¼ï¸ 1. Images\n",
    "\n",
    "Two RGB images captured **back-to-back in time** (roughly 4 FPS apart).\n",
    "\n",
    "- Each image should be:\n",
    "  - Shape: [256, 256, 3] (Height Ã— Width Ã— Channels)\n",
    "  - Type: uint8 (pixel values from 0 to 255)\n",
    "\n",
    "- Before passing into the model, convert the pair into a PyTorch tensor:\n",
    "  - Shape: [1, 3, 2, 256, 256] â†’  [Batch, Channels, Time, Height, Width]\n",
    "  - Type: float32\n",
    "  - Normalize pixel values to [0, 1] by dividing by 255\n",
    "\n",
    "> ðŸ’¡ The model was trained using a **fixed exocentric camera**. Large camera movements may hurt performance.\n",
    "\n",
    "### ðŸ¤– 2. State\n",
    "\n",
    "The robotâ€™s current state, represented as a **7-dimensional vector**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vjepa2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
